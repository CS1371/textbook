<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>

<head>
<title>12_Matrices</title>
<link rel="stylesheet" href="styles/styles.css" />
<script async src="./javascript/index.js"></script>
</head>


<body bgcolor="#ffffff">

<h1 align="center">Chapter 12: Matrices</h1>
<table align="center">
<tbody>
<tr>
<td><a href="11_Plotting.htm">previous</a></td>
<td><a href="Contents.htm">home</a></td>
<td><a href="13_Images.htm">next</a></td>
</tr>
</tbody>
</table>
<ul>
<li><a href="#12_1">12.1   Concept: Behavioral Abstraction</a>
<li><a href="#12_2">12.2  Matrix Operations</a>
<ul>
    <li><a href="#12_2_1">12.2.1  Matrix Multiplication</a>
    <li><a href="#12_2_2">12.2.2  Matrix Division</a>
    <li><a href="#12_2_3">12.2.3  Matrix Exponentiation</a>
</ul>
<li><a href="#12_3">12.3    Implementation</a>
<ul>
    <li><a href="#12_3_1">12.3.1  Matrix Multiplication</a>
    <li><a href="#12_3_2">12.3.2  Matrix Division</a>
</ul>
<li><a href="#12_4">12.4   Rotating Coordinates</a>
<ul>
    <li><a href="#12_4_1">12.4.1 2-D Rotation</a>
    <li><a href="#12_4_2">12.4.2 3-D Rotation</a>
</ul>
<li><a href="#12_5">12.5  Solving Simultaneous Linear   Equations</a>
<ul>
    <li><a href="#12_5_1">12.5.1  Intersecting Lines</a>
</ul>
<li><a href="#12_6">12.6   Engineering Examples</a>
<ul>
    <li><a href="#12_6_1">12.6.1 Ceramic Composition</a>
    <li><a href="#12_6_2">12.6.2 Analyzing an Electrical Circuit</a>
</ul>

<!-- Chapter Objectives -->
<h1>Chapter Objectives</h1>
    <p>This chapter shows matrices as logical extensions of arrays. You will learn about two specialized operations performed with matrices: </p>
    <ul>
        <li>Multiplication for coordinate rotation</li>
        <li>Division for solving simultaneous equations</li>
    </ul>

<!-- Introduction -->
<h1>Introduction</h1>
    <p>Although the matrix operations that are the subject of this chapter can be performed on pairs of vectors or arrays that meet certain criteria, when using these operations, we tend to refer to the data objects as matrices. In most mathematical discussions, the words "matrix" and "array" can be used interchangeably, and rightly so, because they store data in exactly the same form. Moreover, almost all of the operations we can perform on an array can also be performed on a matrix—logical operations, concatenation, slicing, and most of the arithmetic operations behave identically. The fact that some of the mathematical operations are defined differently gives us a chance to think about an important concept that is usually well hidden within the MATLAB language definition.</p>

<!-- Behavioral Abstration -->
<h2><a name="12_1">12.1	Concept: Behavioral Abstraction</a></h2>
    <p>Recall the following concepts:</p>
    <ul>
        <li><em>Abstraction</em> is the ability to ignore specific details and generalize the description of an entity</li>
        <li><em>Data abstraction</em> is the specific example of abstraction that we first considered whereby we could treat vectors of data (and later other collections like structures and arrays) as single entities rather than enumerating their elements individually</li>
        <li><em>Procedural abstraction</em> are functions that collect multiple operations into a form; once they are developed, we can overlook the specific details and treat them as a "black box," much as we treat built-in functions</li>
    </ul>
    <p>Behavioral abstraction combines data and procedural abstraction, encapsulating not only collections of data, but also the operations that are legal to perform on that data. One might argue that this is a new, irrelevant concept best ignored until “we just have to!” However, consider the rules we have had to establish for what we can and cannot do with data collections we have seen so far. For example, am I able to add two arrays together? Yes, but only if they have the same number of rows and columns, or if one of them is scalar. Can I add two character strings? Almost the same answer, except that each string is first converted to a numerical quantity and the result is a vector of numbers and not a string. Can I add two cell arrays? No.</p>
    <p>So at least some, and maybe all, data collections also "understand" the set of operations that are permitted on the data. This encapsulation of data and operations is the essence of behavioral abstraction. Therefore, we distinguish arrays from matrices not by the data they collect, but by the operations that are legal to perform on them.</p>

<!-- Matrix Operations -->
<h2><a name="12_2">12.2	Matrix Operations</a></h2>
    <p>The arithmetic operations that differ between arrays and matrices are multiplication, division, and exponentiation.</p>

<!-- Matrix Multiplication -->
<h3><a name="12_2_1">12.2.1	Matrix Multiplication</a></h3>
    <p>Previously, when we considered multiplying two arrays, we called this scalar multiplication, and it had the following typical array operation characteristics:</p>
    <ul>
        <li>Either the two arrays must be the same size, or one of them must be scalar</li>
        <li>The multiplication was indicated with the .* operator</li>
        <li>The result was an array with the same size as the larger original array</li>
        <li>Each element of the result was the product of the corresponding elements in the original two arrays</li>
    </ul>
    <p>This is best illustrated in Figure 12.1. Scalar division and exponentiation have the same constraints.</p>
    <p>Matrix multiplication, on the other hand, performed using the normal * operator, is an entirely different logical operation, as shown in Figure 12.2. The logical characteristics of matrix multiplication are as follows:</p>
    <ul>
        <li>The two matrices do not have to be the same size.</li>
        <ul>The requirements are either:
            <li>One of the matrices is a scalar, in which case the matrix operation reduces to a scalar multiply.</li>
            <li>The number of columns in the first matrix must equal the number of rows in the second. We refer to these as the <b>inner dimensions</b>. The result is a new matrix with the column count of the first matrix and the row count of the second.</li>
        </ul>
        <li>If, as illustrated, <span class="code">A</span> is an <span class="code">m x n</span> matrix and B is an <span class="code">n x p</span> matrix, the result of <span class="code">A * B</span> is an <span class="code">m x p</span> matrix.</li>
        <li>The item at <span class="code">(i, j)</span> in the result matrix is the sum of the scalar product of the <span class="code">ith</span> row of <span class="code">A</span> and the <span class="code">jth</span> column of <span class="code">B</span>.</li>
        <li>Whereas with scalar multiplication <span class="code">A .* B</span> gives the same result as <span class="code">B .* A</span>, this is not the case with matrix multiplication. In fact, if <span class="code">A * B</span> works, <span class="code">B * A</span> will not work unless both matrices are square, and even then the results are different. (Proof of this can be derived immediately from Figure 12.3 by eliminating the third row and column and exchanging <span class="code">a</span> for <span class="code">b</span>. All four terms of the result of <span class="code">A * B</span> are different from <span class="code">B * A</span>.)</li>
        <li>Whereas with scalar multiplication the original array <span class="code">A</span> can be recovered by dividing the result by <span class="code">B</span>, this is not the case with matrix multiplication unless both matrices are square.</li>
        <li>The <b>identity matrix</b>, sometimes given the symbol <span class="code">I<sub>n</sub></span>, is a square matrix with <span class="code">n</span> rows and <span class="code">n</span> columns that is zero everywhere except on its major diagonal, which contains the value 1. In has the special property that when pre-multiplied by any matrix <span class="code">A</span> with <span class="code">n</span> columns, or post-multiplied with any matrix <span class="code">A</span> with <span class="code">n</span> rows, the result is <span class="code">A</span>. We will need this property to derive matrix division below. (The built-in function <span class="code">eye(...)</span> generates the identity matrix.)</li>
    </ul>
    <p>Figure 12.3 illustrates the mathematics for the case where a 3 x 2 matrix is multiplied by a 2 x 3 matrix, resulting in a 3 x 3 matrix.</p>

<!-- Matrix Division -->
<h3><a name="12_2_2">12.2.2	Matrix Division</a></h3>
<p>Matrix division is the logical process of reversing the effects of a matrix multiplication. The goal is as follows: given <span class="code">A<sub>nxn</sub></span>, <span class="code">B<sub>nxp</sub></span>, and <span class="code">C<sub>nxp</sub></span>, where <span class="code">C = A * B</span>, we wish to define the mathematical equivalent of <span class="code">C/A</span> that will result in <span class="code">B</span>.</p>
<p>Since <span class="code">C = A * B</span>, we are actually searching for some matrix Kn3n by which we can multiply each side of the above equation:</p>
<p><span class="code">K * C = K * A * B</span></p>
<p>This multiplication would accomplish the division we desire if <span class="code">K * A</span> were to result in <span class="code">I<sub>n</sub></span>, the identity matrix. If this were the case, pre-multiplying <span class="code">C</span> by <span class="code"></span> would result in <span class="code">I<sub>n</sub> * B</span>, or simply <span class="code">B</span> by the definition of <span class="code">In</span> above. The matrix <span class="code">K</span> is referred to as the inverse of <span class="code">A</span>, or <span class="code">A<sup>-1</sup></span>. The algebra for computing this inverse is messy but well defined. In fact, Gaussian Elimination to solve linear simultaneous equations accomplishes the same thing. The MATLAB language defines both functions (<span class="code">inv(A)</span>) and operators (“back divide,” \) that accomplish this. However, two things should be noted:</p>
<ul>
    <li>This inverse does not exist for all matrices—if any two rows or columns of a matrix are linearly related, the matrix is <b><em>singular</em></b> and does not have an inverse</li>
    <li>Only non-singular, square matrices have an inverse (just as a set of linear equations is soluble only if there are as many equations as there are unknown variables)</li>
</ul>

<!-- Matrix Exponentiation -->
<h3><a name="12_2_3">12.2.3	Matrix  Exponentiation</a></h3>
<p>For completeness, we mention here that matrix operations include exponentiation. However, this does not suggest that one would encounter <span class="code">A<sub>nxn</sub>^B<sub>nxn</sub></span> in the scope of our applications. Rather, our usage of matrix exponentiation will be confined to <span class="code">A<sup>k</sup></span> where <span class="code">k</span> is any non-zero integer value. The result for positive <span class="code">k</span> is accomplished by multiplying <span class="code">A</span> by itself <span class="code">k</span> times (using matrix multiplication). The result for negative <span class="code">k</span> is accomplished by inverting <span class="code">A<sup>-k</sup></span>. (There is, in fact, meaning in matrix exponentials with non- scalar exponents, but this involves advanced concepts with eigen values and eigenvectors and is beyond the scope of this text.)</p>

<!-- Implementation -->
<h2><a name="12_3">12.3  Implementation</a></h2>
<p>In this section, we see how MATLAB implements matrix multiplication and division. However, since applications that require matrix exponentiation A<sup>k</sup> where k is anything but a scalar quantity are beyond the scope of this text, we will not look at its implementation in MATLAB. </p>

<!-- Matrix Multiplication -->
<h3><a name="12_3_1">12.3.1	Matrix Multiplication</a></h3>
<p>Matrix multiplication is accomplished by using the “normal” multiplication symbol, as illustrated in Exercise 12.1.</p>
<p>In Exercise 12.1 we make the following observations:</p>
<ul>
    <li>Entry 1 creates a 2 x 3 matrix, <span class="code">A</span></li>
    <li>Entry 2 creates a 3 x 1 matrix, <span class="code">B</span>, a column vector</li>
    <li>Entry 3 indicates that this multiplication is legal because the columns in <span class="code">A match the rows in </span>B</span></li>
    <li>Entry 4 shows that, likewise, it is legal to multiply a 1 x 2 vector by a 2 x 3 matrix</li>
    <li>Entry 5 creates an identity matrix</li>
    <li>Entry 6 shows that pre-multiplying <span class="code">A</span> by this is legal because the inner dimensions match</li>
    <li>Entry 7 shows that post-multiplying <span class="code">A</span> by <span class="code">I<sub>2</sub></span> does not work because the inner dimensions do not match</li>
    <li>Entry 8 uses <span class="code">I<sub>3</sub></span> to post-multiply legally</li>
</ul>

<!-- Matrix Division -->
<h3><a name="12_3_2">12.3.2	Matrix Division</a></h3>
<p>Matrix division is accomplished in a number of ways, all of which appear to work, but some give the wrong answer. Returning to the division problem described in Section 12.2.2, we know that <span class="code">A</span> is a square matrix of side <span class="code">n</span>, and <span class="code">B</span> and <span class="code">C</span> have <span class="code">n</span> rows, and <span class="code">C = A * B</span>. If we are actually given the matrices <span class="code">A</span> and <span class="code">B</span>, we can compute <span class="code">B</span> in one of the following ways:</p>
<ul>
    <li><span class="code">B = inv(A) * C</span> — using the MATLAB <span class="code">inv(...)</span> function to compute the inverse of <span class="code">B</span></li>
    <li><span class="code">B = A \ C</span> — "back dividing" <span class="code">B</span> into <span class="code">C</span> to produce the same result</li>
    <li><span class="code">B = C / A</span> — apparently performing the same operation, but <em>giving different answers</em></li>
</ul>
<p>The order in which the matrix multiply is done affects the value of the result; therefore, care must be taken to ensure that the appropriate inversion or division is used. Study the results of Exercise 12.2 carefully.</p>
<p>In Exercise 12.2 we make the following observations:</p>
<ul>
    <li>Entries 1 and 2 construct two 3 x 3 matrices, <span class="code">A</span> and <span class="code">B</span></li>
    <li>Entries 3 and 4 pre-multiply and post-multiply <span class="code">B</span> and <span class="code">A</span>; recall that we expect this to produce different answers</li>
    <li>Entry 5 shows that since we defined <span class="code">inv(B)</span> as that function that produces the result <span class="code">B*inv(B)=I</span>, this should produce a matrix with the same values as <span class="code">A</span></li>
    <li>Entry 6 reveals that normal division by <span class="code">B</span> should also produce a matrix with the same values as <span class="code">A</span></li>
    <li>Entry 7 shows that back dividing <span class="code">B</span> into <span class="code">BA</span> should also produce a matrix equal to <span class="code">A</span></li>
    <li>Entry 8 verifies that dividing <span class="code">BA</span> by <span class="code">B</span> works but does not return the matrix <span class="code">A</span></li>
</ul>

<!-- Rotating Coordinates -->
<h2><a name="12_4">12.4  Rotating Coordinates</a></h2>
<p>A common use for matrix multiplication is for rotating coordinates in two or three dimensions. Previously we have seen the ability to rotate a complete picture by changing the viewing angle. We can move and scale items on a plot by adding coordinate offsets or multiplying them by scalar quantities. However, frequently the need arises to rotate the coordinates of a graphical object by some angle. We can use matrix multiplication to rotate individual items in a picture in two or three dimensions.</p>

<!-- 2-D Rotation -->
<h3><a name="12_4_1">12.4.1	2-D Rotation</a></h3>
<p>The mathematics implementing rotation in two dimensions is relatively straightforward, as shown in Figure 12.4. If the original point location P is (x, y) and you wish to find the point P* (x*, y*) that is the result of rotating P by the angle &theta; about the origin of coordinates, the mathematics are as follows:</p>
<p><span class="code">x* = x cos&theta;  − y sin&theta;</span><br>
<span class="code">y* = x sin&theta;  + y cos&theta;</span></p>
<p>which can be expressed as the matrix equation:</p>
<p><span class="code">P* = A * P</span></p>
<p>where A is found by:</p>
<p>A = [cos&theta; −sin&theta;; sin&theta; cos&theta;]</p>
<p>To rotate the x-y coordinates of a graphic object in the x-y plane about some point, P, other than the origin, you would do as follows:</p>
<ol>
    <li>Translate the object so that P is at the origin by subtracting P from all the object’s coordinates</li>
    <li>Perform the rotation by multiplying each coordinate by the rotation matrix shown above</li>
    <li>Translate the rotated object back to P by adding P to all the rotated coordinates</li>
</ol>
<p><b>Rotating a Line</b> Listing 12.1 illustrates a simple script to rotate a line about the origin.</p>
<p>Figure 12.5 shows the plot resulting from this script.</p>
<p><b>Twinkling Stars</b> As a second example, consider the problem of simulating twinkling stars. One way to accomplish this is to draw two triangles for each star rotating in opposite directions. The script shown in Listing 12.2 accomplishes this.<p>
<p>The results of this script are shown in Figure 12.6.</p>

<!-- 3-D Rotation -->
<h3><a name="12_4_2">12.4.2	3-D Rotation</a></h3>
<p>The mathematics implementing rotation in three dimensions is a natural extension of the 2-D rotation case. We present here a simple way to make this extension. The 2-D rotation in Section 12.4.1 that rotates by the angle &theta; in the x-y plane is actually rotating about the z-axis. If P* and P are now 3-D coordinates, we can rotate P by an angle &theta; about the z-axis with the equation:</p>
<p><span class="code">P* = R<sub>z</sub> * P</span></p>
<p>where R<sub>z</sub> is computed as</p>
<p><span class="code">R<sub>z</sub> =	[cos&theta;, -sin&theta;, 0; sin&theta;, cos&theta;, 0; 0, 0 1]</span></p>
<p>Similarly, we can develop matrices R<sub>x</sub> and R<sub>y</sub> that rotate about the x and y axes by angles &phi; and &psi;, respectively.</p>
<p><span class="code">R<sub>x</sub> =	[1,	0, 0; 0, cos&phi;, -sin&phi;; 0, sin&phi;, cos&phi;]</span></p>
<p><span class="code">R<sub>y</sub> =	[cos&psi;, 0, sin&psi;; 0, 1, 0; -sin&psi;, 0, cos&psi;]<span></p>
<p><span class="code">P* = R<sub>x</sub> * R<sub>y</sub> * R<sub>z</sub> * P</span></p>
<p>An example of a script to rotate the solid cube drawn in Chapter 11 is shown in Listing 12.4. The major problem with rotating solid objects is that the coordinates of the object are defined as arrays of points. However, the rotation matrices need each set of coordinates in single rows. To accomplish this, we will use the <span class="code">reshape(...)</span> function to transform the coordinates to and from the row vectors necessary for the coordinate rotation.</p>
<p>The results after running this script are shown in Figure 12.7. Notice that the mechanization of the top face has caused a "wrapped parcel" effect on the light reflections off that surface.</p>

<!-- Solving Simultaneous Linear Equations -->
<h2><a name="12_5">12.5 Solving Simultaneous Linear Equations</a></h2>
<p>A common use for matrix division is solving simultaneous linear equations. To be solvable, simultaneous linear equations must be expressed as N independent equations involving N unknown variables, x<sub>i</sub>. They are usually expressed in the following form:</p>
<p><span class="code">A<sub>11</sub>x<sub>1</sub> + A<sub>12</sub>x<sub>2</sub> + ... + A<sub>1N</sub>x<sub>N</sub> = c<sub>1</sub></span></p>
<p><span class="code">A<sub>21</sub>x<sub>1</sub> + A<sub>22</sub>x<sub>2</sub> + ... + A<sub>2N</sub>x<sub>N</sub> = c<sub>2</sub></span></p>
<p> ... </p>
<p><span class="code">A<sub>N1</sub>x<sub>1</sub> + A<sub>N2</sub>x<sub>2</sub> + ... + A<sub>NN</sub>x<sub>N</sub> = c<sub>N</sub></span></p>
<p>In matrix form, they can be expressed as follows:</p>
<p><span class="code">A<sub>N x N</sub> = X<sub>N x 1</sub> = C<sub>N x 1</sub></span></p>
<p>from which, since all of the values in A and C are constants, we can immediately solve for the column vector X by back division:</p>
<p><span class="code">X = A\C</span></p>
<p>or by using the matrix inverse function:</p>
<p><span class="code">X = inv(A) * C</span></p>

<!-- Intersecting Lines -->
<h3><a name="12_5_1">12.5.1 Intersecting Lines</a></h3>
<p>A typical example of a simultaneous equation problem might take the following form. Consider two straight lines on a plot with the following general form:</p>
<p><span class="code">A<sub>11</sub>x + A<sub>12</sub>y = c<sub>1</sub></span></p>
<p><span class="code">A<sub>21</sub>x + A<sub>22</sub>y = c<sub>2</sub></span></p>
<p>These lines intersect at some point P (x, y) that is the solution to both of these equations. The equations can be rewritten in matrix form as follows:</p>
<p><span class="code">A * V = c</span></p>
<p>where <span class="code">c</span> is the column vector <span class="code">[c1 c2]'</span> and <span class="code">V</span> is the required result, the column vector <span class="code">[x y]'</span>. The solution is obtained by matrix division as follows:</p>
<p><span class="code">V = A \ c</span></p>
<p>Recall that back divide, like the <span class="code">inv(...)</span> function, will fail to produce a result if the matrix is singular, that is, has two rows or columns that have a linear relationship. In the specific example of two intersecting lines, this singularity occurs when the two lines are parallel, in which case there is no point of intersection. Listing 12.5 shows the solution to a pair of simultaneous equations.</p>
<p>Figure 12.8 shows the result of this script.</p>

<!-- Engineering Examples -->
<h2><a name="12_6">12.6 Engineering Examples</a></h2>
<p>The following examples illustrate applications of the matrix capabilities discussed in this chapter.</p>

<!-- Ceramic Composition -->
<h3><a name="12_6_1">12.6.1 Ceramic Composition</a></h3>
<p>Industrial ceramics plants require mixtures with precise formulations in order to produce products of consistent quality. For example, a factory might require 100 kg of a mix consisting of 67% silica, 5% alumina, 2% calcium oxide, and 26% magnesium oxide. However, the raw material provided is not pure quantities of these materials. Rather, they are delivered as batches of material that consist of the required components in different proportions. Each batch of raw materials is analyzed to determine their composition, and we will need to do the analysis to determine the proportions of the raw materials to mix in order to accomplish the appropriate formulation. The raw materials we will use here are feldspar, diatomite, magnesite, and talc. Table 12.1 illustrates a typical analysis of the composition of these compounds.</p>
<p>For example, if we mixed W<sub>f</sub> kg of feldspar, W<sub>d</sub> kg of diatomite, W<sub>m</sub> kg of magnesite, and W<sub>t</sub> kg of talc, the amount of silica would be 0.695 W<sub>f</sub> + 0.897 W<sub>d</sub> + 0.067 W<sub>m</sub> + 0.692 W<sub>t</sub>. Repeating this equation for the other components produces a matrix equation that reduces to:</p>
<p><span class="code">C = A * W</span></p>
<p>where C is the required composition of the resulting mix, A is a 4 x 4 matrix showing the results of analyzing the four raw materials, and W is the proportions in which should we mix the raw material to produce the desired result. We find the appropriate amounts of the raw material by solving these equations:</p>
<p><span class="code">W = A\B</span></p>
<p>A script that works this problem is shown in Listing 12.6.</p>

<!-- Analyzing an Electrical Circuit -->
<h3><a name="12_6_2">12.6.2 Analyzing an Electrical Circuit</a></h3>
<p>Figure 12.9 illustrates a typical electrical circuit with two voltage sources connected to five resistors with three closed loops. The voltages and resistances are given. We are asked to determine the voltage drop across R1. Solution techniques apply Ohm’s Law to the voltage drops around each closed circuit. When this technique is applied, the equations are as follows:</p>
<p><span class="code">V<sub>1</sub> = i<sub>1</sub> * R<sub>1</sub> + (i<sub>1</sub> – i<sub>2</sub>) * R<sub>4</sub>
<br>0 = i<sub>2</sub> * R<sub>2</sub> + (i<sub>2</sub> – i<sub>3</sub>) * R<sub>5</sub> + (i<sub>2</sub> – i<sub>1</sub>) * R<sub>4</sub>
<br>–V<sub>2</sub> = i<sub>3</sub> * R<sub>3</sub> + (i<sub>3</sub> – i<sub>2</sub>) * R<sub>5</sub></span></p>
<p>When these three equations are manipulated to isolate the three currents, we have the following matrix equation:</p>
<p><span class="code">V = A * I</span></p>
<p>which can be solved as usual by:</p>
<p><span class="code">I = A \ V</span></p>
<p>The script to accomplish this is shown in Listing 12.7.</p>

<!-- Chapter Summary -->
<h3>Chapter Summary</h3>
<p><em>This chapter presented two specialized operations performed with matrices:</em></p>
<ul>
    <li>Matrix multiplication can be used for 2-D and 3-D coordinate rotations by building the appropriate rotation matrices</li>
    <li>Matrix division can be used for solving simultaneous equations by setting up the equations in the general form <span class="code">B = A * x</span>, where the known matrix <span class="code">A</span> is <span class="code">n x n</span> and the known column vector <span class="code">B</span> is <span class="code">n x 1</span>; the unknown vector <span class="code">x</span> is then found by <span class="code">x = A\B</span> or <span class="code">x = inv(A) * B</span></li>
</ul>


<!-- [Special Characters]

[Problems] -->


<table align="center">
<tbody>
<tr>
<td><a href="11_Plotting.htm">previous</a></td>
<td><a href="Contents.htm">home</a></td>
<td><a href="13_Images.htm">next</a></td>
</tr>
</tbody>
</table>

<p align="center"><font size="1">This Web Page was Built with PageBreeze </font><a href="http://pagebreeze.com" target="_blank"><font size="1">Free HTML Editor</font></a><font size="1"> </font></p>
</body>
</html>
