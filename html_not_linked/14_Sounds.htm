<!DOCTYPE HTML>
<html>
<head>
<title>14_Sounds</title>
<script async src="./javascript/index.js"></script>
<!-- include bootstrap -->
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"></script>
<!-- main stylesheet -->
<link rel="stylesheet" href="styles/styles.css" />
</head>

<body>
<div>#top_nav#</div>
<div class="nav-obj">#nav_obj#</div>
<div class="content">
  <h1 align="center" id="14">Chapter 14: Sounds</h1>

  <!-- Chapter Objectives -->
  <h1>Chapter Objectives</h1>
  <div class="container">
  <p>This chapter discusses the following:</p>
      <ul>
          <li>How sound is physically recorded and played back and our internal storage of sound</li>
          <li>Operations that can be performed with the original time trace</li>
          <li>The ability to transform the data into the frequency domain and the physical significance of the transformed data</li>
          <li>Operations that can be performed in the frequency domain</li>
      </ul>
  </div>

  <!--
  <h1>Introduction</h1>
   [not needed here] DMS  -->

   <div class="chp-section" data-sect-num="1" data-sect-name="The Physics of Sound">
    <!-- The Physics of Sound -->
    <h2 id="14_1">14.1	The Physics of Sound</h2>
    <div class="container">
        <p>Any sound source produces sound in the form of pressure fluctuations in the air. While the air molecules move infinitesimal distances in order to propagate the sound, the important part of sound propagation is that pressure waves move rapidly through the air by causing air molecules to “jostle” each other. These pressure fluctuations can be viewed as analog signals—data that have a continuous range of values. These signals have two attributes: their amplitude and their frequency characteristics.</p>
        <p>In absolute terms, sound is measured as the <b>amplitude</b> of pressure fluctuations on a surface like an eardrum or a microphone. However, the challenging characteristic of these data is their dynamic range. Our ears are able to detect small sounds with amplitudes around 10<sup>10</sup> (10 billion) times smaller than the loudest comfortable sound. Sound intensity is therefore usually reported logarithmically, measured in decibels where the intensity of a sound in decibels is calculated as follows:</p>
        <p><code>I<sub>DB</sub> = 10 log<sub>10</sub>(I / I<sub>0</sub>)</code></p>
        <p>where I is the measured pressure fluctuation and I<sub>0</sub> is a reference pressure usually established as the lowest pressure fluctuation a really good ear can detect, 2 x 10<sup>−4</sup> dynes/cm<sup>2</sup>.</p>
        <p>Also, sounds are pressure fluctuations at certain <b>frequencies</b>. The human ear can hear sounds as low as 50 Hz and as high as 20 kHz. Voices on the telephone sound odd because the upper frequency is limited by the telephone equipment to 4 kHz. Typically, hearing damage due to aging or exposure to excessive sound levels causes an ear to lose sensitivity to high and/or low frequencies.</p>
    </div>
  </div>

  <div class="chp-section" data-sect-num="2" data-sect-name="Recording and Playback">
     <!-- Recording and Playback -->
    <h2 id="14_2">14.2	Recording and Playback</h2>
    <div class="container clearfix">
        <div class="float-sm-right card">
            <img src="..\Images\Fig_14_1.JPG" alt="Figure 14.1" class="fig card-img">
            <p class="figure-name card-title">Figure 14.1: Recording and Playback</p>
        </div>
        <p>Early attempts at sound recording concentrated first on mechanical, and later magnetic, methods for storing and reproducing sound. The phonograph/record player depended on the motion of a needle in a groove as a cylinder or disk rotated at constant speed under the playback head. Not surprisingly, when you see the incredible dynamic range required, even the best stereos could not reproduce high-quality sound. Later, analog magnetic tape in various forms replaced the phonograph, offering less wear on the recording and better, but still limited, dynamic range. Digital recording has almost completely supplanted analog recording and will be the subject of this chapter.</p>
        <p>Of course, sound amplitude in analog form is unintelligible to a computer—it must be turned into an electrical signal by a microphone, amplified to suitable voltage levels, digitized, and stored, as shown in Figure 14.1. The key to successful digital recording and playback—whether by digital tape machines, compact disks, or computer files—is the design of the analog-to-digital (A/D) and digital-to-analog (D/A) devices. The reader should remember that this is still low-level data. Each word coming out of the A/D or going into the D/A merely represents the pressure on the microphone at a point in time.</p>
        <p>The primary parameter governing the sound quality is the recording rate—how quickly the mechanism records samples of the sound (the sampling rate). Basic sampling theory suggests that we should use a sampling rate twice the highest frequency you are interested in reproducing, usually around 20,000 samples per second for good music, 5,000 samples per second for speech.</p>
        <div class="float-sm-right card technical-insights">
            <p class="card-title">Technical Insight 14.1</p>
            <p class="card-text">The background theory of sampling is beyond the scope of this text. Interested readers should research Nyquist on a good search engine.</p>
        </div>
        <p>The other parameter, the resolution of the recorded data, has remarkably little effect on the quality of the recording to an untrained ear. The resolution is usually either 8 bits (−128 to 127) or 16 bits (−32,768 to 32767). While 8-bit resolution ought to offer very limited dynamic range, and theoretically should be used only for recording speech, in practice it results in a quality of reproduction for music that is, to an untrained ear, indistinguishable from   that   provided   by   16-bit resolution.</p>
        <p>These parameters must be stored with  any  digital  sound  recording medium and retrieved by the tools that play those sounds. To be able to play such a file, we must receive not only the data stream, but also information indicating the sample frequency, <code>Fs</code>, and the word size.</p>
    </div>
  </div>

  <div class="chp-section" data-sect-num="3" data-sect-name="Implementation">
     <!-- Implementation -->
    <h2 id="14_3">14.3	Implementation</h2>
     <!--[MAJOR REWRITE REQUIRED]-->
     <div class="container">
        <p>MATLAB offers a number of tools for reading sound files: <code>audioread(...)</code>, for example. This function returns two variables: a vector of sound values and the sampling frequency in Hz (samples per second).</p>
        <p>To play a sound file, MATLAB provides the function <code>sound(data, rate)</code> where <code>data</code> is the vector of sound values, and <code>rate</code> is the playback frequency, usually the frequency at which the sound values were recorded. We will see that the function <code>sound(...)</code> passes the data directly to the computer’s sound card, but different implementations will manage the behavior of the software that plays the sound in one of two ways.</p>
        <p><i>Blocking vs. Non-blocking:</i> "Blocking" refers to the behavior of your system after you have called the <code>sound(...)</code> function to play a sound. Blocking players will not return control to the code playing the sound until the sound has completed. This will allow only one sound to be played from an application at a time. Non-blocking players will not wait for the sound card to finish playing the sound, so multiple calls to the <code>sound(...)</code> function will overlay different sounds. You will need to experiment with your particular system to determine whether it blocks or not.</p>
    </div>
     <!--Put these online??A number of .wav files are included on the book’s Companion Web site to demonstrate many aspects of sound files.<-->
   </div>

   <div class="chp-section" data-sect-num="4" data-sect-name="Time Domain Operations">
     <!-- Time Domain Operations -->
    <h2 id="14_4">14.4	Time Domain Operations</h2>
    <div class="container">
        <p>First, we consider three kinds of operations on sound files in the time domain: slicing, playback frequency changes, and sound file frequency changes.</p>
    </div>

    <div class="chp-subsection" data-sub-num="2" data-sub-name="Slicing and Concatenating Sounds">
       <!-- Slicing and Concatenating Sound -->
      <h3 id="14_4_1">14.4.1 Slicing and Concatenating Sound</h3>
      <div class="container">
          <div class="container clearfix">
              <div class="float-sm-right card">
                  <img src="..\Images\Fig_14_2.JPG" alt="Figure 14.2" class="fig card-img">
                  <p class="figure-name card-title">Figure 14.2: Gone With the Wind Speech</p>
              </div>
              <p>Consider the problem of constructing comedic sayings by choosing and assembling words from published speeches. One example of a speech clip is "Frankly, my dear..." from <em>Gone with the Wind</em>. Listing 14.1 describes the process of assembling parts of this speech into a semi-coherent conversation.</p>
              <p>In Listing 14.1, we first read and play the sound from the "frankly, my dear..." speech; the plotted data can be seen in Figure 14.2. Then, we play it louder by increasing the amplitude and then play it softer by decreasing the amplitude. It is then played faster by dropping half the data and then played slower by reducing the playback frequency. Then, all of the speech pieces are pasted together to construct the final sound clip.</p>
          </div>
          <div class="row">
              <div class="col-sm-6">
                  <audio controls><source src="../audio/sp_givdamn2.wav" type="audio/wav" />"Frankly, my dear..." speech</audio>
                  <p class="figure-name">"Frankly, my dear..." speech</p>
              </div>
              <div class="col-sm-6">
                  <audio controls><source src="../audio/sp_bond.wav" type="audio/wav">Bond speech</audio>
                  <p class="figure-name">Bond speech</p>
              </div>
      	</div>
          <div class="row">
              <div class="col-sm-6">
                  <audio controls><source src="../audio/sp_beam.wav" type="audio/wav">Beam speech</audio>
                  <p class="figure-name">Beam Speech</p>
              </div>
              <div class="col-sm-6">
                  <audio controls><source src="../audio/speech.wav" type="audio/wav">Combined speech</audio>
                  <p class="figure-name">Combined Speech</p>
              </div>
          </div>
          <div class="listing">#listing_14_1#</div>
      </div>
    </div>

    <div class="chp-subsection" data-sub-num="2" data-sub-name="Musical Background">
       <!-- Musical Background -->
      <h3 id="14_4_2">14.4.2	Musical Background</h3>
      <div class="container clearfix">
          <div class="float-sm-right card">
              <img src="..\Images\Fig_14_3.JPG" alt="Figure 14.3" class="fig card-img">
              <p class="figure-name card-title">Figure 14.3: Musical Notes</p>
          </div>
          <p>For good historical reasons, music is usually described graphically on a music score. The graphics describe for each note to be played its pitch and its duration, together with other notations indicating how to introduce expression and quality into the music. However, this graphical notation is not amenable to the simple representation of music we need for these experiments. Rather, we will use the representation illustrated in Figure 14.3. The right side of this figure shows a standard piano keyboard, the index of each white note, and the number of half steps necessary to achieve the pitch of each note. On the left side of the figure, we see the method to be used in this text to describe simple tunes. It will consist of an array with two columns and n rows, where n is the number of notes to be played for each tune. The first column is the key number to play, and the second column is the number of beats each note should be played.</p>
          <p>The example to follow in Listing 14.2 will manipulate the file <code>instr_piano.wav</code> to produce a snippet of music. This file is a recording of a single note played on a piano. <!--Other files provided in the Companion Web site are the same note played on a variety of instruments.--> There are two ways to accomplish this, as follows:</p>
          <ol>
           <li>Playing each note at a different playback frequency</li>
           <li>Stretching or shrinking each note to match the required note pitch and playing them all at the same playback frequency</li>
          </ol>
          <p>The first way is easier to understand and code, but very inflexible; the second method is a little more difficult to implement, but completely extensible. Musically speaking, if a sound is played at twice its natural frequency, it is heard as one musical octave higher. When you play a scale by playing each white key in turn from one note to the next octave, there are 8 keys to play with 7 frequency changes: 5 whole note steps (those separated by a black note) and 2 half note steps, for a total of 12 half note steps. These 12 half steps are logarithmically divided where the frequency multiplier between half note steps is 2<sup>1/12</sup>.</p>
          <div class="row">
            <div class="col-sm-6">
              <audio controls><source src="../audio/instr_piano.wav" type="audio/wav">Piano Note</audio>
              <p class="figure-name">Piano Note</p>
            </div>
          </div>
          <div class="listing">#listing_14_2#</div>
          <div class="listing">#alt_Text/ramblinWreck#</div>
      </div>
    </div>

    <div class="chp-subsection" data-sub-num="3" data-sub-name="Changing Sound Frequency">
       <!-- Changing Sound Frequency -->
      <h3 id="14_4_3">14.4.3	Changing Sound Frequency</h3>
      <div class="container clearfix">
          <div class="float-sm-right card">
              <img src="..\Images\Fig_14_4.JPG" alt="Figure 14.4" class="fig card-img">
              <p class="figure-name card-title">Figure 14.4: Creating a Tune File</p>
          </div>
          <p>We will leave as an exercise for the reader the question of playing a tune by changing the playback frequency of each note, which is really never a practical thing to do, and concentrate on playing all the notes of a tune with the same playback frequency. This allows the different notes to be copied into a single sound file and saved to be played back on any digital sound system.</p>
          <p>In order to change the perceived note frequency without changing the playback frequency, we have to change the number of data samples in the original data file much as we stretched or shrunk an image in Section 13.4.1.</p>
          <p><b>Play a Scale</b> Listing 14.2 shows a script that uses this capability to play the C Major scale (all white notes) on the piano. It repeatedly shortens the vector to increase the frequency of the note played.</p>
          <p><b>Play a Simple Tune</b> We now write a script to build a playable <code>.wav</code> file using the note shrinking technique, also shown in Listing 14.2. It reads in the script <code>ramblinWreck</code> to define the tune in which the first column of each cell array specifies the relative pitch (the note on teh scale) and the second the duration in "beats".</p>
          <p>The goal is to put the notes into a single sound array called <code>wreck</code>, as illustrated in Figure 14.4, rather than playing the notes "on the fly." For each part, or instrument, this is accomplished as follows:</p>
          <ul>
           <li>In the <code>play_part</code> function, create an empty array, <code>part</code>, of the appropriate length (the length of the original note plus the total number of beats in the song)</li>
           <li>Initialize <code>where</code> to store the first note at the start of the tune</li>
           <li>Iterate across the <code>part</code> definition array <code>score</code> with the following steps:
               <ul>
                   <li>In the <code>getPitch</code> function, start with the original <code>note</code></li>
                   <li>Get the power of the note to decide how many times to raise the note array by half a step</li>
                   <li>Raise the <code>power</code> to the right pitch and save it as <code>pitch</code></li>
                   <li>Add that <code>pitch</code> vector to the <code>part</code> vector, starting at <code>where</code></li>
                   <li>Move the <code>where</code> variable down the <code>part</code> vector a distance equivalent to the duration of that note</li>
               </ul>
           </li>
           <li>When all the parts have been added to the tune file, play the tune and save it as a <code>.wav</code> file.</li>
          </ul>
      </div>
    </div>
  </div>

  <div class="chp-section" data-sect-num="5" data-sect-name="The Fast Fourier Transform">
    <!-- The Fast Fourier Transform -->
    <h2 id="14_5">14.5  The Fast Fourier Transform</h2>
    <div class="container">
        <p>Typically, the time history display of a sound shows you the amplitude of the sound as a function of time but makes no attempt at showing the frequency content. While this works for the exercises above, we are often more interested in the frequency content of a sound file, for which we need a different presentation—a spectrum display.</p>
    </div>

    <div class="chp-subsection" data-sub-num="1" data-sub-name="Background">
       <!-- Background -->
      <h3 id="14_5_1">14.5.1 Background</h3>
      <div class="container clearfix">
          <div class="float-sm-right card">
              <img src="..\Images\Fig_14_5.JPG" alt="Figure 14.5" class="fig card-img">
              <p class="figure-name card-title">Figure 14.5: A typical spectrum display</p>
          </div>
          <p>In general, a spectrum display shows the amount of sound energy in a given frequency band throughout the duration of the sound analyzed but ignores the time at which the sound at that frequency was generated. Many acoustic amplifiers (see Figure 14.5) include two features that allow you to customize the sound output:</p>
          <ul>
           <li>A spectral display that changes values as the sound is played, indicating the amount of sound energy (vertically) in different frequency bands (horizontally)</li>
           <li>Filter controls to change the relative amplification in different frequency bands</li>
          </ul>
          <div class="float-sm-right card">
              <img src="..\Images\Fig_14_6.JPG" alt="Figure 14.6" class="fig card-img">
              <p class="figure-name card-title">Figure 14.6: Mechanics of the Fourier Transform</p>
          </div>
          <p> In the following paragraphs, we will consider only the analysis of the sound frequency content. The ability to reshape the sound frequency content as the sound plays is beyond the scope of this text.</p>
          <p>To achieve the motion of the spectrum display, software to analyze a segment of the sound file runs periodically and updates the spectrum display. Typically, perhaps 20 times a second, 1/20th second of sound file is analyzed and transformed. The software used for this conversion is known as the Fourier transform.</p>
          <p>While the mathematics of the Fourier transform is beyond the scope of this book, we can make use of the tools it offers without concerning ourselves with the details. There are a number of implementations of this transform; perhaps the most commonly used is the Fast Fourier Transform (FFT).  The  FFT  uses  clever  matrix  manipulations  to  optimize  the algorithm needed to generate the forward (time to frequency) and reverse (frequency to time) transforms.</p>
      </div>
    </div>

    <div class="chp-subsection" data-sub-num="2" data-sub-name="Implementation">
       <!-- Implementation -->
      <h3 id="14_5_2">14.5.2	Implementation</h3>
      <div class="container clearfix">
          <p>Figure 14.6 illustrates the overall process of transforming between the time domain and frequency domain. It starts with a simple sound file, a vector of N sound values in the range (−1.0 to 1.0), which, if played back at a sample frequency <code>Fs</code> samples per second, reproduces the sound. The parameters of interest for characterizing the time trace are:</p>
          <table class="table">
           <tr>
               <td><code>N</code></td>
               <td>the number of samples</td>
           </tr>
           <tr>
               <td><code>F<sub>s</sub></code></td>
               <td>the sampling frequency</td>
           </tr>
           <tr>
               <td><code>&Delta;t</code></td>
               <td>the time between samples, computed as <code>1/Fs</code></td>
           </tr>
           <tr>
               <td><code>T<sub>max</sub></code></td>
               <td>the maximum time is N x &Delta;t</td>
           </tr>
          </table>
          <p>The FFT consumes a file describing the time history of the sound sampled at regular intervals &Delta;t and produces a frequency spectrum with a corresponding set of characteristics. The frequency spectrum consists of the same number, <code>N</code>, of data points, each of which is a complex value with real and imaginary parts. (While many displays actually plot the magnitude of the spectrum values, to accomplish the inverse transform, the complex values must be retained.) The frequency values are "folded" on the plot so that zero frequency occurs at either end of the spectrum, and the maximum frequency occurs in the middle, at spectrum data point <code>N/2</code>.</p>
          <p>The equivalent characteristics for the spectrum data are as follows:</p>
          <table class="table">
           <tr>
               <td><code>N</code></td>
               <td>the number of samples</td>
           </tr>
           <tr>
               <td><code>&Delta;f</code></td>
               <td>the frequency difference between samples, computed as <code>1/T<sub>max</sub></code></td>
           </tr>
           <tr>
               <td><code>F<sub>max</sub></code></td>
               <td>the frequency value at the end of the plot, is <code>N x &Delta;f</code> However, since the mathematics force this frequency to actually replicate the beginning frequency, the maximum effective frequency actually occurs at the mid-point with value <code>F<sub>max</sub>/2</code>.</td>
           </tr>
          </table>
          <div class="float-sm-right card technical-insights">
              <p class="card-title">Technical Insight 14.2</p>
              <p class="card-text">The fact that the actual maximum frequency is half of the sampling frequency is consistent with the Nyquist criterion that the maximum frequency you can discern with digital sampling is half the sampling frequency.</p>
          </div>
          <p>The FFT is mechanized using the function <code>fft(...)</code>, which consumes the time history and produces the complex spectrum file. The inverse FFT  function, <code>ifft(...)</code>,  takes  a spectrum array and reconstructs the time history. This pair of functions provides a powerful set of tools for manipulating sound files.</p>
      </div>
    </div>

    <div class="chp-subsection" data-sub-num="3" data-sub-name="Simple Spectral Analysis">
       <!-- Simple Spectral Analysis -->
      <h3 id="14_5_3">14.5.3	Simple Spectral Analysis</h3>
      <div class="container clearfix">
          <div class="float-sm-right card">
              <img src="..\Images\Fig_14_7.JPG" alt="Figure 14.7" class="fig card-img">
              <p class="figure-name card-title">Figure 14.7: FFT of a sine wave</p>
          </div>
          <p>Listing 14.3 illustrates a script that creates 10 seconds of an 8 Hz sine wave, plots the first second of it, performs the FFT, and plots the real and imaginary parts of the spectrum. Notice the following:</p>
          <ul>
           <li>A sine wave in the time domain transforms to a line in the frequency domain because all its energy is concentrated at that frequency—8 Hz in this example.</li>
           <li>Since the FFT is a linear process, multiple sine or cosine waves added together at different frequencies have additive effects in the spectrum.</li>
           <li>The resulting spectrum is complex (with real and imaginary parts) and symmetrical about its center, the point of maximum frequency. On the plot, of course, one cannot make the frequency axis labels reduce from the center to the end.</li>
           <li>The real part of the spectrum is mirrored about the center; the imaginary part is mirrored and inverted (the complex conjugate of the original data).</li>
           <li>The phase of the complex spectrum retains the position of the sine wave in the time domain—it would be totally real for a cosine wave symmetrically placed in time and totally imaginary for a sine wave in the same relationship.</li>
          </ul>
          <p>The script in Listing 14.3 creates three sub-plots: the original sine wave and then the amplitude and phase of the spectrum.</p>
          <p>Figure 14.7 shows the result from running this script. It confirms the earlier statement that the real part of the spectrum is mirrored about the center frequency, and the imaginary part is mirrored and inverted.</p>
          <div class="listing">#listing_14_3#</div>
      </div>
    </div>
  </div>

  <div class="chp-section" data-sect-num="6" data-sect-name="Frequency Domain Operations">
     <!-- Frequency Domain Operations -->
    <h2 id="14_6">14.6  Frequency Domain Operations</h2>
    <div class="container">
      <p>This section is intended to set up the process of building a synthesizer that will replicate with a reasonable level of fidelity the sound of a piano without resorting, as we did above, to reading and manipulating a piano time history. To do this, we must understand the basics of the frequency spectrum of the instrument and shaping its time history.</p>
    	<p>As a typical example of operating in the frequency domain, we will consider analyzing the spectral quality of different musical instruments. The intent of this section is to develop a plot showing the spectra of a selection of different musical instruments. <!--We will first build a function that plots the spectrum for a single instrument and then build the script to create all the plots.--> Listing 14.4 shows a function that reads the .wav file of an instrument. The author is deeply indebted to the University of Miami's Audio and Signal Processing Laboratory who gave permission to use their musical instrument sound collection.  This provided us with a series of sound traces recording different instruments.  All were recorded in a quiet room playing the same musical note giving us clean, pure recordings of a large number of instruments. http://chronos.ece.miami.edu/~dasp/samples/samples.html. <!--All the instruments are carefully playing a note at about 260 Hz.--></p>
      <div class="row">
          <div class="col-sm-6">
              <audio controls><source src="../audio/instr_violin.wav" type="audio/wav">Violin Frequency Spectrum</audio>
              <p class="figure-name">Violin Frequency Spectrum</p>
          </div>
          <div class="col-sm-6">
              <audio controls><source src="../audio/instr_tpt.wav" type="audio/wav">Trumpet Frequency Spectrum</audio>
              <p class="figure-name">Trumpet Frequency Spectrum</p>
          </div>
      </div>
      <div class="row">
        <div class="col-sm-6">
            <audio controls><source src="../audio/trainwhistle.wav" type="audio/wav">Train Whistle Frequency Spectrum</audio>
            <p class="figure-name">Train Whistle Frequency Spectrum</p>
        </div>
      </div>
      <div class="container clearfix">
          <div class="float-sm-right card">
              <img src="..\Images\Fig_14_8.JPG" alt="Figure 14.8" class="fig card-img">
              <p class="figure-name card-title">Figure 14.8: Instrument Spectra</p>
          </div>
          <p>The results are shown in Figure 14.8. It is interesting to notice the following:</p>
          <ul>
            <li>None of the instruments produce a pure tone. The lowest frequency at which there is energy is usually called the fundamental frequency, and successive peaks to the right at multiples of the fundamental frequency are referred to, for example, as the first, second, and third harmonics.</li>
            <li>Several instruments have much more energy in the harmonics than in the fundamental frequency.</li>
            <li>"Families" of instruments have similar spectral shapes—the strings, for example, have strong fundamental and second harmonic energy. In principle, these characteristic spectral "signatures" can be used to synthesize the sound of instruments, and even to identify individual instruments when played in groups.</li>
          </ul>
      </div>
      <div class="listing">#listing_14_4#</div>
    </div>

      <div class="chp-subsection" data-sub-num="1" data-sub-name="Details of the Spectral Data">
      <h3 id="14_6_1">14.6.1 Details of the Spectral Data</h3>
          <div class="container clearfix">
              <div class="float-sm-right card">
                  <img src="..\Images\Fig_14_9.JPG" alt="Figure 14.9" class="fig card-img">
                  <p class="figure-name card-title">Figure 14.9: Instrument Spectra</p>
              </div>
              <p>We will now look carefully at the characteristics of the Frequency Spectrum using the code in Listing 14.5.  The first view we create is the spectrum plot of a sine wave at 261.6 Hz which musicians would recognize as Middle C.  Since all the sound energy is at that one frequency, the spectrum is a vertical line at 261.6 Hz.  The scaling on the vertical axis is complicated and irrelevant to this discussion. Consider the vertical axis to show the relative amounts of energy as a function of frequency. Note that the FFT function is assuming that the sound being analyzed is the sum of a number of sine waves at different frequencies.  The task of the FFT is to separate out and display the sound energy at each frequency.  In this example, there is only one frequency.</p>
          </div>
          <div class="container clearfix">
              <div class="float-sm-right card">
                  <img src="..\Images\Fig_14_10.JPG" alt="Figure 14.10" class="fig card-img">
                  <p class="figure-name card-title">Figure 14.10: Spectrum of a trumpet</p>
              </div>
              <p>In this next example, we will plot the absolute values of the frequency spectrum of a trumpet playing the same note at 261.6 Hz. Notice some interesting characteristics of this spectrum:
          		<ul>
            		<li>There is some energy at 261.6 Hz (normally called the "fundamental frequency" - the note intended to to be played by the musician.)  </li>
            		<li>However, the bulk of the sound energy is at higher frequencies.</li>
            		<li>"Musical" sounds in general are composed of energy at the fundamental frequency combined with energy at integer multipliers of the fundamental frequency referred to as the "harmonics." Contrast this with the "dirty" looking frequency spectrum of the train whistle in Figure 14.8 that is not normally classed as "musical"</li>
                <li>		Notice with the trumpet that the maximum energy is in the 4th harmonic and the first ten harmonics all have more energy than the fundamental.  This gives the trumpet its "brilliant" sound.</li>
          		</ul></p>
          </div>
          <div class="container clearfix">
              <div class="float-sm-right card">
                  <img src="..\Images\Fig_14_11.JPG" alt="Figure 14.11" class="fig card-img">
                  <p class="figure-name card-title">Figure 14.11: The Spectrum is actually complex</p>
              </div>
              <p>In this next example, we will plot the real and imaginary parts of the same trumpet frequency spectrum. Notice that the physical mechanics of the shape of the trumpet has created frequency spectrum values that are "out of phase" with other spectrum values.  We will not really be concerned with this except to observe that we need to reproduce the phase as well as the magnitude of the energy if we are to produce a faithful reproduction of the spectrum of an instrument.</p>
          </div>
          <div class="listing">#listing_14_5#</div>
        </div>

        <div class="chp-subsection" data-sub-num="2" data-sub-name="Notes fading with time">
          <h3 id="14_6_2">14.6.2 Notes fading with time</h3>
          <div class="container clearfix">
              <div class="float-sm-right card">
                  <img src="..\Images\Fig_14_12.JPG" alt="Figure 14.12" class="fig card-img">
                  <p class="figure-name card-title">Figure 14.12: trumpet time history</p>
              </div>
              <p>Many instruments have a relatively constant amplitude when played.  For example, a trumpet's amplitude is controlled by the musician's breath, and could be assumed to be constant over the duration of the note as illustrated in Figure 14.12. If this is the case, everything necessary to make a synthesizer has been accomplished. Listing 14.6 was used to generate this figure, together with the frequency spectrum not repeated here.</p>
          </div>
          <div class="listing">#listing_14_6#</div>
          <div class="container clearfix">
              <div class="float-sm-right card">
                  <img src="..\Images\Fig_14_13.JPG" alt="Figure 14.13" class="fig card-img">
                  <p class="figure-name card-title">Figure 14.13: piano time history</p>
              </div>
              <p>Some instruments, however, are played by striking a string, for example, and the allowing the reverberation of the sound to continue, gradually fading over time.  Figure 14.13 illustrates the changes over time when a piano note is played. In order to apply a profile over time to our synthetic piano note, we need to reach ahead to Chapter 15 to use curve fitting to model the shape of the sound decay.  We will merely report the general steps here and refer the reader to section 15.2 for the details.
              <ul>
                <li>In general, we need first to define some data points shown as red plus signs that define the decay.</li>
                <li>We then use the tools of chapter 15 to compute a small number of coefficients that force a polynomial curve to best fit these data points shown as the cyan colored curve.  </li>
                <li>Finally, we generate that curve and multiply the sound at each sample point by the interpolated value of that curve.</li>
              </ul>
            </p>
          </div>
          <div class="listing">#listing_14_7#</div>
        </div>
      </div>


    <div class="chp-section" data-sect-num="7" data-sect-name="Engineering Example - Music Synthesizer">
       <!-- Engineering Example -->
      <h2 id="14_7">14.7 Engineering Example - Music Synthesizer</h2>
      <div class="container">
          <p>A music synthesizer is an electronic instrument with a piano style keyboard that is able to simulate the sound of multiple instruments. Unlike the instrument sounds we have used so far, the instrument sounds are not stored as large time histories. Rather, they are stored as the Fourier coefficients  similar to those illustrated in Figure 14.11. The sound is then reconstructed by multiplying sin or cosine waves of the right frequency by the stored coefficients. For some instruments, this is sufficient. Other instruments such as pianos need to have the amplitude of the resulting sound modified to match a typical profile. Listings 14.8 illustrates a possible technique for extracting the most important Fourier coefficients from the piano sound. We would then apply the fading curve created in section 14.6.2 above to each note and create the music from the individual notes.</p>
          <div class="listing">#listing_14_8#</div>
          <div class="row">
              <div class="col-sm-6">
                  <audio controls><source src="..\audio\humoresque_1.wav" type="audio/wav">Anton Dvorak: "Humoresque"</audio>
                  <p class="figure-name">Anton Dvorak: "Humoresque"</p>
              </div>
          </div>
      </div>
    </div>

  <!-- Chapter Summary -->
  <h2>Chapter Summary</h2>
  <div class="container">
      <p>This chapter presented the following:</p>
      <ul>
          <li>Sounds are read with specific readers that provide a time history and sampling frequency</p>
          <li>Sounds can be played through the computer’s sound system and saved to disk as a sound file ready for playing on any digital player</li>
          <li>We can slice and concatenate sounds to edit speeches and change the frequency of the sound to change its pitch</li>
          <li>We can analyze the frequency content of sound using the Fast Fourier Transform (FFT)</li>
          <li>We can modify the spectra by adding, deleting, or changing the sound levels at chosen frequencies under certain controlled conditions</li>
          <li>We can reconstruct a sound from the FFT coefficients.</li>
      </ul>
  </div>
<h2>Self Test</h2>
<div class="container">

Use the following questions to check your understanding of the material in this
chapter:
<h3>True or False</h3>
<ol>
<li>Playing a sound file at double the recorded sample frequency raises
its pitch by an octave.</li>
          <li>Removing every other sample from a sound file lowers the pitch by
an octave.</li>
          <li>The resolution of the recorded data has a significant effect on the
quality of the recording.</li>
          <li>After performing an FFT, the zero frequency occurs at either end of
the spectrum and the maximum frequency occurs in the middle.</li>
          <li>Since the mathematics of the FFT are linear, the spectrum of a sound
added in the time domain is also added in the frequency domain.</li>
</ol>
<h3>Fill in the Blanks</h3>
<ol>
<li>Sound pressure fluctuations have two attributes: their
_______________ and their _______________.</li>
          <li>Each word coming out of the _____________ or going into the
_______________ merely represents the _______________ on the
microphone at a point in time.</li>
          <li>The steps from one note to the next higher octave are divided into
_______________ increments: _______________ whole note steps and
_______________ half note steps, for a total of _______________ half
note steps.</li>
          <li>A spectrum display shows the amount of _______________ in a
given _______________ throughout the duration of the sound
analyzed.</li>
</ol>

<h3>Programming Projects</h3>
<ol>
<li>These are fundamental exercises with sound files. You should not
hard-code any of the answers for this problem, and you should not
need iteration.
<ol type="a">
<li>Select and read a suitable .wav file, and save the sound values
and sampling frequency.</li>
          <li>Create a new sound that has double the frequency of the original
sound, and store your answer in the variable sound_Double.</li>
          <li>Create a new sound that is the same as the original except that
the pitch is raised by five half tones. Store your answer in the
variable raised_pitch.</li>
          <li>We need a figure showing two views each of these three
sounds, created using subplot. <br>
In the left column, plot the
original sound, sound_Double, and raised_pitch, labeling each
plot accordingly.<br>
In the right column, plot the first quarter of the values of the
power spectrum of each sound with the proper frequency values
on the horizontal axis.</li>
          <li>Play each of the sounds in the following order: original sound,
sound_Double , and raised_pitch each at the original sampling
frequency.</li>
          </ol></li>
          <li>Write a function that will accept a string specifying a sound file and
do the following:
<ol type="a">
<li>Play back the sound..</li>
          <li>Plot the sound in the time domain, titling and labeling your plot
appropriately..</li>
          <li>Compute the frequency with the most energy in this file.
Validate your answer by plotting the lower quarter of the
frequencies of the Fourier Transform of the sound. Don't forget
that the Fourier Transform is complex; you will need to reason
with and plot the absolute value of the spectrum..</li>
          <li>Test this function with suitable .wav files.</li>
		  </ol>
          <li>Write a function named plotSound that takes in the name of a sound
file and produces a 1 3 2 figure with two plots. The first plot should
be a plot of the sound in the time domain. The second plot should
be a plot of the sound in the frequency domain. Your function
should not return anything. Label the first plot 'Time Domain' and
label its axes appropriately. Label second plot 'Frequency Domain'
and label its axes appropriately.
The Time Domain plot should be an amplitude vs. time plot. For
simplicity make sure that your time vector starts at dt (delta time)
and goes to n*dt (t max ) where n is the number of samples.
<br>The Frequency Domain plot should be a power vs. frequency plot
where power is the absolute value of the FFT of the amplitude
values. For simplicity make sure that your frequency vector starts at
df (delta frequency) and goes to n*df (2*f max ).</li>
          <li>In this exercise, we will write a script to create an instrument sound
from scratch.
<ol type="a">
<li>Create a vector, t , of time values from 0 to 2 seconds with length
40,000 samples.</li>
          <li>Convert the frequency of middle C (261.6 Hz) to v radians per
second.</li>
          <li>Compute a sound sample as cos(vt) over the range of t in part a.</li>
          <li>Play that sound at a sample frequency of 20,000, and verify that it
sounds "about right."</li>
          <li>Perform the Fourier Transform on the sound vector, establish the
correct axis values, and prove that the sound is exactly Middle C.
</ol></li>
          <li>Write a function named playNote that takes in a string representing
a note on the piano. Your function should return a vector
representing the amplitude values of the note in addition to the
correct sampling frequency to be used to play it back. You should
do this by modifying the sound in the provided instr_piano.wav file
which is Middle C played on the piano. Note that the returned
sampling frequency should be the same as that in instr_piano..wav .
Here is a list of all the possible note names representing notes that
your function should work with and below that is the number of
half steps above/below the middle C for that note:
<table>
<tr><td >cn</td><td>cn#</td><td>dn</td><td>dn#</td><td>en</td><td>fn</td><td>fn#</td><td>gn</td><td>gn#</td><td>a(n+1)</td><td>a(n+1)#</td><td>b(n+1)</td><td>c(n+1)</td></tr>
<tr><td>-12</td><td>-11</td><td>-10</td><td>-9</td><td>-8</td><td>-7</td><td>-6</td><td>-5</td><td>-4</td><td>-3</td><td>-2</td><td>-1</td><td>0</td></tr>
</table>
where c4 is the middle C, c5 is 1 octave above it, and c3 is 1 octave
below it. Similarly, f5 is 1 octave higher than f4, etc. For example,
[y1 fs] = playNote('c5'); should return a vector such that
sound(y1, fs) should sound like middle C</li>
          <li>Finally, you will use these tools to play your favorite song.
		  <ol type = "a">
<li>Find the music for your favorite song, and translate it into the
symbology of Problem 14.5.</li>
          <li> Write a script that uses the playNote function to play your song
on the piano.</li>
          <li>Modify playNote to use your synthetic instrument from Problem
14.3, and save it as playSynthetic .</li>
          <li>Write a script that uses playSynthetic to play your song in
futuristic style.
</ol>
</ol>
</div>
</div>


</body>
</html>
